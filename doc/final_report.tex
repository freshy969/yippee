\documentclass[11pt, letterpaper, oneside, twocolumn]{article}
\usepackage{fancyvrb}
\usepackage{float}
\usepackage{subfig}
\usepackage{graphicx,dblfloatfix}
\usepackage[hmargin=1in, vmargin=1in]{geometry}               
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{arrows,backgrounds,calc,fit,mindmap,positioning,trees,shapes}

% \usepackage{fullpage}
% \usepackage{xunicode,xltxtra,url,parskip} 	%other packages for formatting

% \addtolength{\topmargin}{-.5in}
% \addtolength{\bottommargin}{-.875in}
% \addtolength{\textheight}{1.75in}

\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\DefineVerbatimEnvironment{code}{Verbatim}{fontsize=\small}

\newcommand{\tab}{\hspace*{2em}}
\floatstyle{plain}
\restylefloat{figure}
\begin{document}
\title{Yippee: Web Search for the New Millenium}
\author{	TJ Du, Chris Imbriano, Margarita Miranda, Nikos Vasilakis\\
	\{tdu2, imbriano, mmiran, nvas\}@seas.upenn.edu}
\date{May 8, 2012}

\maketitle

\begin{figure*}[ht]
\begin{tikzpicture}
  \path[mindmap,concept color=darkgray,text=white]
    node[concept] {Yippee Search Engine}
    [clockwise from=0]
    child[concept color=teal!] {
      node[concept] {Pastry Node 1 + BerkeleyDB}
      [clockwise from=90]
      child [concept color=cyan!] { node[concept] (c1) {Crawler} }
      child [concept color=purple!]  { node[concept] (i1) {Indexer} }
      child [concept color=lime!] { node[concept] (s1) {Search} }
    }  
    child[concept color=teal!] {
      node[concept] {Pastry Node 2 + BerkeleyDB}
      [clockwise from=-10]
      child [concept color=cyan!] { node[concept] (c2) {Crawler} }
      child [concept color=purple!] { node[concept] (i2) {Indexer} }
      child [concept color=lime!] { node[concept] (s2) {Search} }
    }
    child[concept color=teal!] { node[concept] {Pastry Node 3 + BerkeleyDB} }
    child[concept color=magenta!] {
      node[concept] {PageRank}
      [clockwise from=-90]
      child { node[concept] {Hadoop} }
    }
    child[concept color=teal!] { node[concept] {Pastry Node 4 + BerkeleyDB} };

    \begin{pgfonlayer}{background}
        % \draw [circle connection bar ] (i1) edge [color=violet!50!] (i2);
        % \draw[-to,shorten >=10pt,gray] (c1) -- (c2);
        \draw[very thick, dashed, double distance=2pt] (3,2) arc (31:90:3cm);
        \draw[very thick, dashed, double distance=2pt] (3.8,-1.9) arc (-15:-50:1cm);
        \draw[very thick, dashed, double distance=2pt] (0,-4.0) arc (-80:-100:1cm);
        %\node[anchor=north,text width=10cm,inner sep=.05cm,align=center,fill=white] at (2,2) {Pastry}; text width=3cm,
        \node[fill=white, inner sep=0.1cm, text centered] at (2,3) {Pastry};
    \end{pgfonlayer}
\end{tikzpicture}
  \caption{Yippee's fully distributed architecture. Nodes communicate
  via FreePastry}
\end{figure*}

\section{ Introduction }

To say the world wide web is enormous would be an understatement.  
In the last month (March 2012),  Google's index size was estimated to be between forty-five and fifty-five billion pages.\cite{websize}
The task of presenting a web user with relevant pages according to keyword search has proven to be a difficult one, as many of the commercial search engines during the dawn of the web were unable to return their own site for a search for their site's name.\cite{google} 
Google has shown that besides the load speed of the results, page relevance is just as critical a metric of an effective search engine. 

Yippee is a distributed search engine designed to provide fast, relevant results to keyword queries via the web.  

\section{Project Approach}
\label{sec:approach}

The Yippee Search Engine is a distributed application comprised of 4 main components: a web crawler to recursively download pages from the web, an indexer to index crawled web pages, our simplistic implementation of Google's PageRank algorithm \cite{pagerank}, and a search module to retrieve documents based on user-provided keywords.  
The crawler, indexer and search components are distributed via the FreePastry peer-to-peer substrate, while the PageRank components is centralized.  All parts of the project are deployed to Amazon Elastic Cloud Computing (EC2) instances.  

The architecture of the Yippee is shown in figure \textbf{Update this graph} 1 \textbf{make this number a reference since there will be more figures this time}.


Section \ref{sec:component} will describe each component and its relevant design decision.
Section \ref{sec:evaluation} will present evaluations for some of the design decisions. \textbf{rephrase}

\textbf{These paragraphs probably belong in component discussion}




\label{sec:SOAR} %software architecture
\begin{figure}[!b]
  \centering
  \includegraphics[scale=0.45]{figures/yippee_map.pdf}
  \caption{Software Architecture.}
\end{figure}



\section{Component Discussion}
\label{sec:component}



\subsection{Infrastructure and Software Engineering}

* Build process - ant, EntryPoint, Configuration
* Testing - JUnit
* Persistent Storage - BerkeleyDB
* Distributed Application - FreePastry 

\subsection{Crawler}

The Yippee crawler is a multithreaded distributed crawler based on the design of the Mercator crawler\cite{mercator}. 
Each of the Yippee node is responsible for a subset of the web as determined by FreePastry key-based routing keyed on the host of each url discovered.
Mercator uses various implementations of the URLFrontier to fine tune the crawlers performance and politeness.
For this reason, we made our frontier pluggable via a factory pattern and interface.
This allowed us to easily test frontiers with varying degrees of politeness and performance.

One criticism of the Mercator design\cite{ubi,para} is the centralized nature of the URLFrontier. 

\textbf{Needs to be edited}
Our crawler follows the Mercator design; however, since the design is
criticized for its centralized URLFrontier , we made this
component pluggable. That  way, we can easily plug in different implementations 
and analyze the  performance. Raw content  is  written to a persistent, shared queue  
between the crawler and  the indexer, enabling
asynchronous indexing and page-rank.

\subsection{Indexer}

\textbf{Needs to be edited}
The Yippee Indexer has a multi-threaded and distributed design. The Indexer on each node creates IndexerWorker threads that then perform all work. All IndexerWorkers poll from a queue of documents that are placed into the BerkeleyDB DocAug database by the Crawler. JTidy is used to parse each document. For every word in the document, the Porter Stemmer algorithm is used and the stemmed word is compared to a Lexicon of stemmed words and a regular expression to look for numbers. If the word is valid, a Hit object is created. The Hit object contains important information about that word in relation to the document: document id, document length, position, and word decoration (bold, italics). Similarly, any anchor hit on a document gets created as both a Hit for that document and an AnchorHit for the document its link refers to. An AnchorHit object is an extension of the Hit object and includes a field for the document it points to. When an anchor is reached, this means a link is reached. The link of the current document getting indexed and the link found get logged to a special file that will later get used for PageRank. After all the Hit and AnchorHit objects are created for that document, two things happen. First, a DocEntry object is created and saved to the DocEntry Database. This object contains valuable information about the document needed pre and post query such as page rank, tf-idf, document url, and document title. Second, all the Hits for that document are sent to a centralized NodeIndex that handles them.

The NodeIndex accumulates all the Hits into a HashMap that stores the word as its key and an ArrayList of Hits for that word from all IndexWorkers. The Hits will get saved in a Barrel database on the node whose id is closest to the Hitâ€™s SHA-1 hashed word. FreePastry is used for passing the ArrayList of Hits for a word to the appropriate node. In order to reduce the number of messages being passed around the pastry ring, messages were sent in batches. For every 10 documents whose Hits were sent to the NodeIndex, the current HashMap would get iterated over and only words whose ArrayList of Hits were longer than 3 would get sent to the ring to get saved in the Barrels.

The Barrels contain an inverted index and is managed by a BarrelManager. There is a BarrelManager on each node that is responsible for the HitLists of all the words whose ids get hashed to it. There is only one HitList object for every word. A HitList contains the word, document frequency, and two HashMaps: one for Hit term frequency and another for AnchorHit term frequency in a particaular document. The keys for the HashMaps are the document ids and the values are floats. Upon message arrival, a HitList object is either created or updated depending on whether that word has been seen before. Then document frequency and term frequency are adjusted accordingly. This is performed at this point in order to reduce the number of calculations done post query search.  


\subsection{PageRank}

The PageRank component of Yippee is the only centralized part of the system.
It performs a calculation based-on \[Brin, Page 1999\]\cite{pagerank} to determine the relative importance of each page in the index.
The PageRank calculation is an iterative MapReduce running on a single-node Hadoop cluster after retrieving, from each Yippee node, a log of links found during the indexing process.
We decided that PageRank should not be a distributed component for the following reasons. 
First, the PageRank metric is only relevant when doing searches and since the Yippee Search Engine would only be in deployment for a short period of time, it was only necessary to run PageRank, in production, once.
Second, given the frequency of PageRank jobs, implementing a distributed version introduced complexity that was unwarranted, so the simpler solution was chosen.


\subsection{Search}
\textbf{Needs to be edited}
The final piece of Yippee involves presenting the user with highly relevant pages given a provided keyword.
Once a keyword search is received, a heuristic aggregator will calculate highly relevant documents given queries to the indexed data.
The precise details of the heuristic aggregator have yet to be determined, but the main idea is to shrink the pool of possible results as much as possible using metrics like TF/IDF, word count and position, etc.
For these relevant pages, the PageRank score will be taking into account to calculate a final ordering of results to be returned.

\section{Evaluation}
\label{sec:evaluation}

\section{Future Work}
\label{sec:future}

\begin{enumerate}
\item Spell-check
\item Ability to add more nodes/remove nodes on the fly
\item User feedback on query results
\end{enumerate}


\textbf{edit}
If we have additional time, we will implement a spell-check and AJAX support for users to give feedback on query results.


\section{Conclusion}
\label{sec:conculsion}

\section{ Division of Labor }
\label{sec:labor}

While each member of the group is ultimately responsible for monitoring progress and completion of one of the four components described in Section 2, no individual is tasked with its implementation.
Instead, the group collectively determines the high level architecture and component interfaces, then a pair of members implements their assigned component.
The other two group members will write black box tests of the agreed upon interface without inspecting the source code such that their tests are unbiased towards any particular implementation decision.
The sign-off responsibility is divided as follows: Crawler - Nikos, Indexer - Margarita, PageRank - Chris, Search Engine and Web UI - TJ.

For the first milestone, Chris and Nikos will work on the Crawler and the preliminary components of FreePastry, and TJ and Margarita will work on the Indexer.
The same approach will be used for PageRank and User Interface but the team members may be shuffled.

\begin{thebibliography}{9}

  \bibitem{websize} WorldWideWebSize.com, 11 Apr 2012 $\langle$http://www.worldwidewebsize.com$\rangle$
  \bibitem{google} Sergey Brin and Lawrence Page \emph{The Anatomy of a Large-Scale Hypertextual Web Search Engine}, Stanford University, 1999
  \bibitem{pagerank} Sergey Brin and  Lawrence Page and Rajeev Motwani and Terry Winograd \emph{The PageRank Citation Ranking: Bringing Order to the Web}, Stanford InfoLab, 1999
  \bibitem{mercator} Allan Heydon and Marc Najork, \emph{Mercator: A scalable, extensible web crawler}, Compaq Systems Research Center, September 26, 2001
  \bibitem{ubi} Paolo Boldi, Bruno Codenotti, Massimo Santini, Sebastiano Vigna, \emph{UbiCrawler: a scalable fully distributed Web crawler}, Software: Practice and Experience 34 (2004), 711-726.
  \bibitem{para} Junghoo Cho, H\'{e}ctor Garcia-Molina, \emph{Parallel crawlers}, Proc. of the 11th International Conference on World Wide Web, 2002.


\end{thebibliography}

\end{document}
