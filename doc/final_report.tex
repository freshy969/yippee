\documentclass[11pt, letterpaper, oneside, twocolumn]{article}
\usepackage{fancyvrb}
\usepackage{float}
\usepackage{subfig}
\usepackage{graphicx,dblfloatfix}
\usepackage[hmargin=1in, vmargin=1in]{geometry}               
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{arrows,backgrounds,calc,fit,mindmap,positioning,trees,shapes}
\usepackage{xcolor}

% \usepackage{fullpage}
% \usepackage{xunicode,xltxtra,url,parskip} 	%other packages for formatting

% \addtolength{\topmargin}{-.5in}
% \addtolength{\bottommargin}{-.875in}
% \addtolength{\textheight}{1.75in}

\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\DefineVerbatimEnvironment{code}{Verbatim}{fontsize=\small}

\newcommand{\tab}{\hspace*{2em}}
\floatstyle{plain}
\restylefloat{figure}
\begin{document}
\title{Yippee: Web Search for the New Millenium}
\author{	TJ Du, Chris Imbriano, Margarita Miranda, Nikos Vasilakis\\
	\{tdu2, imbriano, mmiran, nvas\}@seas.upenn.edu}
\date{May 8, 2012}

\maketitle

\begin{figure*}[ht]
\begin{tikzpicture}
  \path[mindmap,concept color=darkgray,text=white]
    node[concept] {Yippee Search Engine}
    [clockwise from=0]
    child[concept color=teal!] {
      node[concept] {Pastry Node 1 + BerkeleyDB}
      [clockwise from=90]
      child [concept color=lightgray!] { node[concept] (c1) {Crawler} }
      child [concept color=purple!]  { node[concept] (i1) {Indexer} }
      child [concept color=olive!] { node[concept] (s1) {Search} }
    }  
    child[concept color=teal!] {
      node[concept] {Pastry Node 2 + BerkeleyDB}
      [clockwise from=-10]
      child [concept color=lightgray!] { node[concept] (c2) {Crawler} }
      child [concept color=purple!] { node[concept] (i2) {Indexer} }
      child [concept color=olive!] { node[concept] (s2) {Search} }
    }
    child[concept color=teal!] { node[concept] {Pastry Node 3 + BerkeleyDB} }
    child[concept color=brown!] {
      node[concept] {PageRank}
      [clockwise from=-90]
      child { node[concept] {Hadoop} }
    }
    child[concept color=teal!] { node[concept] {Pastry Node 4 + BerkeleyDB} };

    \begin{pgfonlayer}{background}
        % \draw [circle connection bar ] (i1) edge [color=violet!50!] (i2);
        % \draw[-to,shorten >=10pt,gray] (c1) -- (c2);
        \draw[very thick, dashed, double distance=2pt] (3,2) arc (31:90:3cm);
        \draw[very thick, dashed, double distance=2pt] (3.8,-1.9) arc (-15:-50:1cm);
        \draw[very thick, dashed, double distance=2pt] (0,-4.0) arc (-80:-100:1cm);
        %\node[anchor=north,text width=10cm,inner sep=.05cm,align=center,fill=white] at (2,2) {Pastry}; text width=3cm,
        \node[fill=white, inner sep=0.1cm, text centered] at (2,3) {Pastry};
    \end{pgfonlayer}
\end{tikzpicture}
  \caption{Yippee's fully distributed architecture. Nodes communicate
  via FreePastry}
\end{figure*}

\section{ Introduction }

To say the world wide web is enormous would be an understatement.  
In the last month (March 2012),  Google's index size was estimated to be between forty-five and fifty-five billion pages.\cite{websize}
The task of presenting a web user with relevant pages according to keyword search has proven to be a difficult one, as many of the commercial search engines during the dawn of the web were unable to return their own site for a search for their site's name.\cite{google} 
Google has shown that besides the load speed of the results, page relevance is just as critical a metric of an effective search engine. 

Yippee is a distributed search engine designed to provide fast, relevant results to keyword queries via the web.  

\section{Project Approach}
\label{sec:approach}

The Yippee Search Engine is a distributed application comprised of 4 main components: a web crawler to recursively download pages from the web, an indexer to index crawled web pages, our simplistic implementation of Google's PageRank algorithm \cite{pagerank}, and a search module to retrieve documents based on user-provided keywords.  
The crawler, indexer and search components are distributed via the FreePastry peer-to-peer substrate, while the PageRank components is centralized.  All parts of the project are deployed to Amazon Elastic Cloud Computing (EC2) instances.  

The architecture of the Yippee is shown in figure \textbf{Update this graph} 1 \textbf{make this number a reference since there will be more figures this time}.


Section \ref{sec:component} will describe each component and its relevant design decision.
Section \ref{sec:evaluation} will present evaluations for some of the design decisions. \textbf{rephrase}


\label{sec:SOAR} %software architecture
\begin{figure}[!b]
  \centering
  \includegraphics[scale=0.45]{figures/yippee_map.pdf}
  \caption{Software Architecture.}
\end{figure}



\section{Component Discussion}
\label{sec:component}



\subsection{Infrastructure and Software Engineering}

We used four Extra Large (m1.xlarge) Amazon instances, each comprising of 8 4-core CPUs, 15GB of RAM and attached 1 TB EBS Volumes. 
Yippee was implemented using Java 1.6 and the user-interface servlet adhered to the 2.4 specification. 
Ant (v. 1.8) and bash scripts were extensively used to automate the build, test and deployment process. 
We implemented using test-driven development\footonote{We used jUnit 4, instead of 3, mainly due to flexible fixtures.} and pair-programmed, and each team which was responsible for a component employed white-box testing while the other team did black-box testing (and vice versa). 
During the development of the project we made use of git (v. 1.7.4). 
Our current LOC is slightly above 14k.

For our persistent storage across yippee, we used BerkeleyDB. 
It was used both as a persistent storage and as an asynchronous queue between components as a crawler and the indexer; that way, the crawler could start and stop at any time without affecting the indexer and page-rank.  

In order to route data across multiple nodes, we made use of FreePastry, a Pasrty implementation. 
Pastry is an overlay and routing network for the implementation of a distributed hash table(DHT) similar to Chord we did in class. 
All packages share a common configuration singleton object, which makes it easy to access get and receive messages of different priority. 
Each message is sent by the module responsible for routing messages, and handled accordingly upon retrieval.

\subsection{Crawler}

The Yippee crawler is a multithreaded distributed crawler based on the design of the Mercator crawler\cite{mercator}. 
Each of the Yippee node is responsible for a subset of the web as determined by FreePastry key-based routing keyed on the host of each url discovered.
Mercator uses various implementations of the URLFrontier to fine tune the crawlers performance and politeness.
The crawler is multithreaded and requires very little inter-thread communication, but the URLFrontier can be a bottle neck depending on the implementation.
For this reason, we made our frontier pluggable via a factory pattern to allow us to easily test frontiers with varying degrees of politeness and implementations.


The Yippee Crawler has three important persistence objects; DocAug, FrontierSavedState and RobotsTxt.
DocAugs are wrapper objects for the contents of the crawled documents.
They are an object stored by the crawler and later read by the indexer
FrontierSavedStates store the necessary data to checkpoint the state of a URLFrontier and allow for restarting a frontier without losing the URLs waiting in the queue.
Last is the RobotsTxt object which stores information retrieved from the robots.txt file for a given host.
The RobotsTxt interface is defined such that the object can take a url as a parameter and determine if the robots.txt policy allows crawling.


\textbf{Needs to be edited}
Our crawler follows the Mercator design; however, since the design is
criticized for its centralized URLFrontier , we made this
component pluggable. That  way, we can easily plug in different implementations 
and analyze the  performance. Raw content  is  written to a persistent, shared queue  
between the crawler and  the indexer, enabling
asynchronous indexing and page-rank.

\subsection{Indexer}

\textbf{Needs to be edited}
The Yippee Indexer has a multi-threaded and distributed design loosely based on \[Brin, Page 1999\]\cite{pagerank}. 
The Indexer on each node creates IndexerWorker threads that then perform all work. 
All IndexerWorkers poll from a queue of documents that are placed into the BerkeleyDB DocAug database by the Crawler. 
JTidy is used to parse each document. 
For every word in the document, the Porter Stemmer algorithm is used and the stemmed word is compared to a Lexicon of stemmed words and a regular expression to look for numbers. 
If the word is valid, a Hit object is created. 
The Hit object contains important information about that word in relation to the document: document id, document length, position, and word decoration (bold, italics). 
Similarly, any anchor hit on a document gets created as both a Hit for that document and an AnchorHit for the document its link refers to. 
An AnchorHit object is an extension of the Hit object and includes a field for the document it points to. 
The decision to create both a Hit and an AnchorHit for an anchor was based on the fact that the word tells very important information about both documents. 
At this point the link of the current document getting indexed and the link found get logged to a special file that will later get used for PageRank. 
After all the Hit and AnchorHit objects are created for that document, two things happen. 
First, a DocEntry object is created and saved to the DocEntry Database. 
This object contains valuable information about the document needed pre and post query such as page rank, tf-idf, document url, and document title. 
Second, all the Hits for that document are sent to a centralized NodeIndex that handles them.


The NodeIndex accumulates all the Hits into a HashMap that stores the word as its key and an ArrayList of Hits for that word from all IndexWorkers. 
The Hits will get saved in a Barrel database on the node whose id is closest to the Hitâ€™s SHA-1 hashed word. 
FreePastry is used for passing the ArrayList of Hits for a word to the appropriate node. 
In order to reduce the number of messages being passed around the pastry ring, messages were sent in batches. 
For every 10 documents whose Hits were sent to the NodeIndex, the current HashMap would get iterated over and only words whose ArrayList of Hits were longer than 3 would get sent to the ring to get saved in the Barrels. 
These values were chosen based on finding a balance with the frequency and size of the Pastry messages being sent. 


The Barrels contain an inverted index and is managed by a BarrelManager. 
There is a BarrelManager on each node that is responsible for the HitLists of all the words whose ids get hashed to it. There is only one HitList object for every word. 
A HitList contains the word, document frequency, and two HashMaps: one for Hit term frequency and another for AnchorHit term frequency in a particaular document. 
The keys for the HashMaps are the document ids and the values are floats. 
Upon message arrival, a HitList object is either created or updated depending on whether that word has been seen before. 
Then document frequency and term frequency are adjusted accordingly. 
This is performed at this point in order to reduce the number of calculations done post query search.  


\subsection{PageRank}

The PageRank component of Yippee is the only centralized part of the system.
It performs a calculation based-on \[Brin, Page 1999\]\cite{pagerank} to determine the relative importance of each page in the index.
The PageRank calculation is an iterative MapReduce running on a single-node Hadoop cluster after retrieving, from each Yippee node, a log of links found during the indexing process.
We decided that PageRank should not be a distributed component for the following reasons. 
First, the PageRank metric is only relevant when doing searches and since the Yippee Search Engine would only be in deployment for a short period of time, it was only necessary to run PageRank, in production, once.
Second, given the frequency of PageRank jobs, implementing a distributed version introduced complexity that was unwarranted, so the simpler solution was chosen.


\subsection{Search}
\textbf{Needs to be edited}
The final piece of Yippee involves presenting the user with highly relevant pages given a provided keyword.
Once a keyword search is received, a heuristic aggregator will calculate highly relevant documents given queries to the indexed data.
The precise details of the heuristic aggregator have yet to be determined, but the main idea is to shrink the pool of possible results as much as possible using metrics like TF/IDF, word count and position, etc.
For these relevant pages, the PageRank score will be taking into account to calculate a final ordering of results to be returned.

\section{Evaluation}
\label{sec:evaluation}

The Indexer performed best with 5 IndexerWorker threads at a rate of 250 documents per minute. 
This is timed from when the document is polled from the queue by the IndexerWord to when the Hits are saved to the appropriate Barrels. 
Increasing the number of threads resulted in Pastry messages getting sent out more frequently and overflowing the node queues. 
There was also overhead in writing to the DocEntry database between the threads. 
Despite this, the performance rate over time was constant meaning overflow of messages was eliminated. 

\section{Future Work}
\label{sec:future}

\begin{enumerate}
\item Spell-check
\item Ability to add more nodes/remove nodes on the fly
\item User feedback on query results
\end{enumerate}


\textbf{edit}
If we have additional time, we will implement a spell-check and AJAX support for users to give feedback on query results.


\section{Conclusion}
\label{sec:conculsion}

\section{ Division of Labor }
\label{sec:labor}

While each member of the group is ultimately responsible for monitoring progress and completion of one of the four components described in Section 2, no individual is tasked with its implementation.
Instead, the group collectively determines the high level architecture and component interfaces, then a pair of members implements their assigned component.
The other two group members will write black box tests of the agreed upon interface without inspecting the source code such that their tests are unbiased towards any particular implementation decision.
The sign-off responsibility is divided as follows: Crawler - Nikos, Indexer - Margarita, PageRank - Chris, Search Engine and Web UI - TJ.

For the first milestone, Chris and Nikos will work on the Crawler and the preliminary components of FreePastry, and TJ and Margarita will work on the Indexer.
The same approach will be used for PageRank and User Interface but the team members may be shuffled.

\begin{thebibliography}{9}

  \bibitem{websize} WorldWideWebSize.com, 11 Apr 2012 $\langle$http://www.worldwidewebsize.com$\rangle$
  \bibitem{google} Sergey Brin and Lawrence Page \emph{The Anatomy of a Large-Scale Hypertextual Web Search Engine}, Stanford University, 1999
  \bibitem{pagerank} Sergey Brin and  Lawrence Page and Rajeev Motwani and Terry Winograd \emph{The PageRank Citation Ranking: Bringing Order to the Web}, Stanford InfoLab, 1999
  \bibitem{mercator} Allan Heydon and Marc Najork, \emph{Mercator: A scalable, extensible web crawler}, Compaq Systems Research Center, September 26, 2001
  \bibitem{ubi} Paolo Boldi, Bruno Codenotti, Massimo Santini, Sebastiano Vigna, \emph{UbiCrawler: a scalable fully distributed Web crawler}, Software: Practice and Experience 34 (2004), 711-726.
  \bibitem{para} Junghoo Cho, H\'{e}ctor Garcia-Molina, \emph{Parallel crawlers}, Proc. of the 11th International Conference on World Wide Web, 2002.


\end{thebibliography}

\end{document}
